{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lifelines\n",
      "  Using cached https://files.pythonhosted.org/packages/39/8b/239479f5c4317fe92ccc9e8690b60fa6d4613125eae2c03062356b4c32dd/lifelines-0.25.5-py3-none-any.whl\n",
      "Requirement already satisfied: pandas>=0.23.0 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from lifelines) (0.23.4)\n",
      "Requirement already satisfied: patsy>=0.5.0 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from lifelines) (0.5.1)\n",
      "Collecting autograd-gamma>=0.3 (from lifelines)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from lifelines) (1.16.3)\n",
      "Collecting autograd>=1.3 (from lifelines)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from lifelines) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=3.0 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from lifelines) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from pandas>=0.23.0->lifelines) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from pandas>=0.23.0->lifelines) (2018.7)\n",
      "Requirement already satisfied: six in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from patsy>=0.5.0->lifelines) (1.12.0)\n",
      "Requirement already satisfied: future>=0.15.2 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from autograd>=1.3->lifelines) (0.17.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines) (2.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zoli\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=3.0->lifelines) (40.6.3)\n",
      "Installing collected packages: autograd, autograd-gamma, lifelines\n",
      "Successfully installed autograd-1.3 autograd-gamma-0.5.0 lifelines-0.25.5\n"
     ]
    }
   ],
   "source": [
    "!pip install lifelines --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 0\n",
      "{'mb_size': 64, 'iteration': 1000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 50, 'num_layers_shared': 5, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'METABRIC/results/'}\n",
      "METABRIC/results//itr_0 (a:1.0 b:5.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [144, 288, 432]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m85.6396\u001b[0m\n",
      "updated.... average c-index = 0.7871\n",
      "Current best: 0.7871249804184858\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 1\n",
      "{'mb_size': 64, 'iteration': 1000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 50, 'num_layers_shared': 3, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'METABRIC/results/'}\n",
      "METABRIC/results//itr_0 (a:1.0 b:1.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [144, 288, 432]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m19.4927\u001b[0m\n",
      "updated.... average c-index = 0.7787\n",
      "Current best: 0.7871249804184858\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This runs random search to find the optimized hyper-parameters using cross-validation\n",
    "\n",
    "INPUTS:\n",
    "    - OUT_ITERATION: # of training/testing splits\n",
    "    - RS_ITERATION: # of random search iteration\n",
    "    - data_mode: mode to select the time-to-event data from \"import_data.py\"\n",
    "    - seed: random seed for training/testing/validation splits\n",
    "    - EVAL_TIMES: list of time-horizons at which the performance is maximized; \n",
    "                  the validation is performed at given EVAL_TIMES (e.g., [12, 24, 36])\n",
    "\n",
    "OUTPUTS:\n",
    "    - \"hyperparameters_log.txt\" is the output\n",
    "    - Once the hyper parameters are optimized, run \"summarize_results.py\" to get the final results.\n",
    "'''\n",
    "import time, datetime, os\n",
    "import get_main\n",
    "import numpy as np\n",
    "\n",
    "import import_data as impt\n",
    "\n",
    "\n",
    "# this saves the current hyperparameters\n",
    "def save_logging(dictionary, log_name):\n",
    "    with open(log_name, 'w') as f:\n",
    "        for key, value in dictionary.items():\n",
    "            f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "# this open can calls the saved hyperparameters\n",
    "def load_logging(filename):\n",
    "    data = dict()\n",
    "    with open(filename) as f:\n",
    "        def is_float(input):\n",
    "            try:\n",
    "                num = float(input)\n",
    "            except ValueError:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key,value = line.strip().split(':', 1)\n",
    "                if value.isdigit():\n",
    "                    data[key] = int(value)\n",
    "                elif is_float(value):\n",
    "                    data[key] = float(value)\n",
    "                elif value == 'None':\n",
    "                    data[key] = None\n",
    "                else:\n",
    "                    data[key] = value\n",
    "            else:\n",
    "                pass # deal with bad lines of text here    \n",
    "    return data\n",
    "\n",
    "\n",
    "# this randomly select hyperparamters based on the given list of candidates\n",
    "def get_random_hyperparameters(out_path):\n",
    "    SET_BATCH_SIZE    = [32, 64, 128] #mb_size\n",
    " \n",
    "    SET_LAYERS        = [1,2,3,5] #number of layers\n",
    "    SET_NODES         = [50, 100, 200, 300] #number of nodes\n",
    "\n",
    "    SET_ACTIVATION_FN = ['relu', 'elu', 'tanh'] #non-linear activation functions\n",
    "\n",
    "    SET_ALPHA         = [0.1, 0.5, 1.0, 3.0, 5.0] #alpha values -> log-likelihood loss \n",
    "    SET_BETA          = [0.1, 0.5, 1.0, 3.0, 5.0] #beta values -> ranking loss\n",
    "    SET_GAMMA         = [0.1, 0.5, 1.0, 3.0, 5.0] #gamma values -> calibration loss\n",
    "\n",
    "    new_parser = {'mb_size': SET_BATCH_SIZE[np.random.randint(len(SET_BATCH_SIZE))],\n",
    "\n",
    "                 'iteration': 1000,\n",
    "\n",
    "                 'keep_prob': 0.6,\n",
    "                 'lr_train': 1e-4,\n",
    "\n",
    "                 'h_dim_shared': SET_NODES[np.random.randint(len(SET_NODES))],\n",
    "                 'h_dim_CS': SET_NODES[np.random.randint(len(SET_NODES))],\n",
    "                 'num_layers_shared':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n",
    "                 'num_layers_CS':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n",
    "                 'active_fn': SET_ACTIVATION_FN[np.random.randint(len(SET_ACTIVATION_FN))],\n",
    "\n",
    "                 'alpha':1.0, #default (set alpha = 1.0 and change beta and gamma)\n",
    "                 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n",
    "                 'gamma':0,   #default (no calibration loss)\n",
    "                 # 'alpha':SET_ALPHA[np.random.randint(len(SET_ALPHA))],\n",
    "                 # 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n",
    "                 # 'gamma':SET_GAMMA[np.random.randint(len(SET_GAMMA))],\n",
    "\n",
    "                 'out_path':out_path}\n",
    "    \n",
    "    return new_parser #outputs the dictionary of the randomly-chosen hyperparamters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### MAIN SETTING\n",
    "OUT_ITERATION               = 1\n",
    "RS_ITERATION                = 2\n",
    "\n",
    "data_mode                   = 'METABRIC'\n",
    "seed                        = 1234\n",
    "\n",
    "\n",
    "##### IMPORT DATASET\n",
    "'''\n",
    "    num_Category            = typically, max event/censoring time * 1.2 (to make enough time horizon)\n",
    "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
    "    max_length              = maximum number of measurements\n",
    "    x_dim                   = data dimension including delta (num_features)\n",
    "    mask1, mask2            = used for cause-specific network (FCNet structure)\n",
    "\n",
    "    EVAL_TIMES              = set specific evaluation time horizons at which the validatoin performance is maximized. \n",
    "    \t\t\t\t\t\t  (This must be selected based on the dataset)\n",
    "\n",
    "'''\n",
    "if data_mode == 'SYNTHETIC':\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = impt.import_dataset_SYNTHETIC(norm_mode = 'standard')\n",
    "    EVAL_TIMES = [12, 24, 36]\n",
    "elif data_mode == 'METABRIC':\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = impt.import_dataset_METABRIC(norm_mode = 'standard')\n",
    "    EVAL_TIMES = [144, 288, 432] \n",
    "else:\n",
    "    print('ERROR:  DATA_MODE NOT FOUND !!!')\n",
    "\n",
    "\n",
    "DATA = (data, time, label)\n",
    "MASK = (mask1, mask2) #masks are required to calculate loss functions without for-loops.\n",
    "\n",
    "out_path      = data_mode + '/results/'\n",
    "\n",
    "for itr in range(OUT_ITERATION):\n",
    "    \n",
    "    if not os.path.exists(out_path + '/itr_' + str(itr) + '/'):\n",
    "        os.makedirs(out_path + '/itr_' + str(itr) + '/')\n",
    "\n",
    "    max_valid = 0.\n",
    "    log_name = out_path + '/itr_' + str(itr) + '/hyperparameters_log.txt'\n",
    "\n",
    "    for r_itr in range(RS_ITERATION):\n",
    "        print('OUTER_ITERATION: ' + str(itr))\n",
    "        print('Random search... itr: ' + str(r_itr))\n",
    "        new_parser = get_random_hyperparameters(out_path)\n",
    "        print(new_parser)\n",
    "\n",
    "        # get validation performance given the hyperparameters\n",
    "        tmp_max, model,tr_data,te_data, tr_time,te_time, tr_label,te_label,tr_mask1,te_mask1, tr_mask2,te_mask2 = get_main.get_valid_performance(DATA, MASK, new_parser, itr, EVAL_TIMES, MAX_VALUE=max_valid)\n",
    "\n",
    "        if tmp_max > max_valid:\n",
    "            max_valid = tmp_max\n",
    "            max_parser = new_parser\n",
    "            save_logging(max_parser, log_name)  #save the hyperparameters if this provides the maximum validation performance\n",
    "\n",
    "        print('Current best: ' + str(max_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EPSILON = 1e-08\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "# import sys\n",
    "\n",
    "from termcolor import colored\n",
    "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import import_data as impt\n",
    "import utils_network as utils\n",
    "\n",
    "from class_DeepHit import Model_DeepHit\n",
    "from utils_eval import c_index, brier_score, weighted_c_index, weighted_brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-70271a321920>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel_DeepHit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DeepHit\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_settings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_dims' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "##### PREDICTION & EVALUATION\n",
    "saver.restore(sess, in_path + '/itr_' + str(out_itr) + '/models/model_itr_' + str(out_itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### PREDICTION\n",
    "    pred = model.predict(te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##### MAIN SETTING\n",
    "OUT_ITERATION               = 5\n",
    "\n",
    "data_mode                   = 'SYNTHETIC' #METABRIC, SYNTHETIC\n",
    "seed                        = 1234\n",
    "\n",
    "EVAL_TIMES                  = [1,1,20000000] # evalution times (for C-index and Brier-Score)\n",
    "num_Event = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11061"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1yr c_index</th>\n",
       "      <th>1yr c_index</th>\n",
       "      <th>2yr c_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Event_1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1yr c_index  1yr c_index  2yr c_index\n",
       "Event_1         -1.0         -1.0         -1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11061\n",
      "1\n",
      "11061\n",
      "20000000\n",
      "11061\n",
      "ERROR: evaluation horizon is out of range\n",
      "========================================================\n",
      "--------------------------------------------------------\n",
      "- C-INDEX: \n",
      "         1yr c_index  1yr c_index  20000000yr c_index\n",
      "Event_1         -1.0         -1.0                -1.0\n",
      "--------------------------------------------------------\n",
      "- BRIER-SCORE: \n",
      "          1yr B_score   1yr B_score  20000000yr B_score\n",
      "Event_1  4.536369e-08  4.536369e-08                -1.0\n",
      "========================================================\n",
      "1\n",
      "11061\n",
      "1\n",
      "11061\n",
      "20000000\n",
      "11061\n",
      "ERROR: evaluation horizon is out of range\n",
      "========================================================\n",
      "--------------------------------------------------------\n",
      "- C-INDEX: \n",
      "         1yr c_index  1yr c_index  20000000yr c_index\n",
      "Event_1         -1.0         -1.0                -1.0\n",
      "--------------------------------------------------------\n",
      "- BRIER-SCORE: \n",
      "          1yr B_score   1yr B_score  20000000yr B_score\n",
      "Event_1  4.536369e-08  4.536369e-08                -1.0\n",
      "========================================================\n",
      "1\n",
      "11061\n",
      "1\n",
      "11061\n",
      "20000000\n",
      "11061\n",
      "ERROR: evaluation horizon is out of range\n",
      "========================================================\n",
      "--------------------------------------------------------\n",
      "- C-INDEX: \n",
      "         1yr c_index  1yr c_index  20000000yr c_index\n",
      "Event_1         -1.0         -1.0                -1.0\n",
      "--------------------------------------------------------\n",
      "- BRIER-SCORE: \n",
      "          1yr B_score   1yr B_score  20000000yr B_score\n",
      "Event_1  4.536369e-08  4.536369e-08                -1.0\n",
      "========================================================\n",
      "1\n",
      "11061\n",
      "1\n",
      "11061\n",
      "20000000\n",
      "11061\n",
      "ERROR: evaluation horizon is out of range\n",
      "========================================================\n",
      "--------------------------------------------------------\n",
      "- C-INDEX: \n",
      "         1yr c_index  1yr c_index  20000000yr c_index\n",
      "Event_1         -1.0         -1.0                -1.0\n",
      "--------------------------------------------------------\n",
      "- BRIER-SCORE: \n",
      "          1yr B_score   1yr B_score  20000000yr B_score\n",
      "Event_1  4.536369e-08  4.536369e-08                -1.0\n",
      "========================================================\n",
      "1\n",
      "11061\n",
      "1\n",
      "11061\n",
      "20000000\n",
      "11061\n",
      "ERROR: evaluation horizon is out of range\n",
      "========================================================\n",
      "--------------------------------------------------------\n",
      "- C-INDEX: \n",
      "         1yr c_index  1yr c_index  20000000yr c_index\n",
      "Event_1         -1.0         -1.0                -1.0\n",
      "--------------------------------------------------------\n",
      "- BRIER-SCORE: \n",
      "          1yr B_score   1yr B_score  20000000yr B_score\n",
      "Event_1  4.536369e-08  4.536369e-08                -1.0\n",
      "========================================================\n",
      "========================================================\n",
      "- FINAL C-INDEX: \n",
      "         1yr c_index  1yr c_index  20000000yr c_index\n",
      "Event_1         -1.0         -1.0                -1.0\n",
      "--------------------------------------------------------\n",
      "- FINAL BRIER-SCORE: \n",
      "          1yr B_score   1yr B_score  20000000yr B_score\n",
      "Event_1  4.536369e-08  4.536369e-08                -1.0\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "for out_itr in range(OUT_ITERATION):    \n",
    "    ### EVALUATION\n",
    "    result1, result2 = np.zeros([num_Event, len(EVAL_TIMES)]), np.zeros([num_Event, len(EVAL_TIMES)])\n",
    "\n",
    "    for t, t_time in enumerate(EVAL_TIMES):\n",
    "        print(t_time)\n",
    "        eval_horizon = int(t_time)\n",
    "        print(num_Category)\n",
    "\n",
    "        if eval_horizon >= num_Category:\n",
    "            print( 'ERROR: evaluation horizon is out of range')\n",
    "            result1[:, t] = result2[:, t] = -1\n",
    "        else:\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n",
    "            for k in range(num_Event):\n",
    "                # result1[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "                # result2[k, t] = brier_score(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "                result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "                result2[k, t] = weighted_brier_score(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "    FINAL1[:, :, out_itr] = result1\n",
    "    FINAL2[:, :, out_itr] = result2\n",
    "\n",
    "    ### SAVE RESULTS\n",
    "    row_header = []\n",
    "    for t in range(num_Event):\n",
    "        row_header.append('Event_' + str(t+1))\n",
    "\n",
    "    col_header1 = []\n",
    "    col_header2 = []\n",
    "    for t in EVAL_TIMES:\n",
    "        col_header1.append(str(t) + 'yr c_index')\n",
    "        col_header2.append(str(t) + 'yr B_score')\n",
    "\n",
    "    # c-index result\n",
    "    df1 = pd.DataFrame(result1, index = row_header, columns=col_header1)\n",
    "    #df1.to_csv(in_path + '/result_CINDEX_itr' + str(out_itr) + '.csv')\n",
    "\n",
    "    # brier-score result\n",
    "    df2 = pd.DataFrame(result2, index = row_header, columns=col_header2)\n",
    "    #df2.to_csv(in_path + '/result_BRIER_itr' + str(out_itr) + '.csv')\n",
    "\n",
    "    ### PRINT RESULTS\n",
    "    print('========================================================')\n",
    "#    print('ITR: ' + str(out_itr+1) + ' DATA MODE: ' + data_mode + ' (a:' + str(alpha) + ' b:' + str(beta) + ' c:' + str(gamma) + ')' )\n",
    "#    print('SharedNet Parameters: ' + 'h_dim_shared = '+str(h_dim_shared) + ' num_layers_shared = '+str(num_layers_shared) + 'Non-Linearity: ' + str(active_fn))\n",
    "#    print('CSNet Parameters: ' + 'h_dim_CS = '+str(h_dim_CS) + ' num_layers_CS = '+str(num_layers_CS) + 'Non-Linearity: ' + str(active_fn)) \n",
    "\n",
    "    print('--------------------------------------------------------')\n",
    "    print('- C-INDEX: ')\n",
    "    print(df1)\n",
    "    print('--------------------------------------------------------')\n",
    "    print('- BRIER-SCORE: ')\n",
    "    print(df2)\n",
    "    print('========================================================')\n",
    "\n",
    "\n",
    "    \n",
    "### FINAL MEAN/STD\n",
    "# c-index result\n",
    "df1_mean = pd.DataFrame(np.mean(FINAL1, axis=2), index = row_header, columns=col_header1)\n",
    "df1_std  = pd.DataFrame(np.std(FINAL1, axis=2), index = row_header, columns=col_header1)\n",
    "#df1_mean.to_csv(in_path + '/result_CINDEX_FINAL_MEAN.csv')\n",
    "#df1_std.to_csv(in_path + '/result_CINDEX_FINAL_STD.csv')\n",
    "\n",
    "# brier-score result\n",
    "df2_mean = pd.DataFrame(np.mean(FINAL2, axis=2), index = row_header, columns=col_header2)\n",
    "df2_std  = pd.DataFrame(np.std(FINAL2, axis=2), index = row_header, columns=col_header2)\n",
    "#df2_mean.to_csv(in_path + '/result_BRIER_FINAL_MEAN.csv')\n",
    "#df2_std.to_csv(in_path + '/result_BRIER_FINAL_STD.csv')\n",
    "\n",
    "\n",
    "### PRINT RESULTS\n",
    "print('========================================================')\n",
    "print('- FINAL C-INDEX: ')\n",
    "print(df1_mean)\n",
    "print('--------------------------------------------------------')\n",
    "print('- FINAL BRIER-SCORE: ')\n",
    "print(df2_mean)\n",
    "print('========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, num_Event, num_Category  = np.shape(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FINAL1 = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "FINAL2 = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITR: 1 DATA MODE: METABRIC (a:1.0 b:5.0 c:0)\n",
      "INFO:tensorflow:Restoring parameters from METABRIC/results//itr_0/models/model_itr_0\n",
      "========================================================\n",
      "ITR: 1 DATA MODE: METABRIC (a:1.0 b:5.0 c:0)\n",
      "SharedNet Parameters: h_dim_shared = 50 num_layers_shared = 5Non-Linearity: <function relu at 0x00000233B4F837B8>\n",
      "CSNet Parameters: h_dim_CS = 50 num_layers_CS = 2Non-Linearity: <function relu at 0x00000233B4F837B8>\n",
      "--------------------------------------------------------\n",
      "- C-INDEX: \n",
      "         287yr c_index  288yr c_index  432yr c_index\n",
      "Event_1       0.709845       0.709845       0.758947\n",
      "--------------------------------------------------------\n",
      "- BRIER-SCORE: \n",
      "         287yr B_score  288yr B_score  432yr B_score\n",
      "Event_1       0.026749       0.026751        0.04494\n",
      "========================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'METABRIC/results//itr_1/hyperparameters_log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-80fa91e95662>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mout_itr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOUT_ITERATION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0min_hypfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/itr_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_itr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/hyperparameters_log.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0min_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_logging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_hypfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-80fa91e95662>\u001b[0m in \u001b[0;36mload_logging\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_logging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mis_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'METABRIC/results//itr_1/hyperparameters_log.txt'"
     ]
    }
   ],
   "source": [
    "_EPSILON = 1e-08\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "# import sys\n",
    "\n",
    "from termcolor import colored\n",
    "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import import_data as impt\n",
    "import utils_network as utils\n",
    "\n",
    "from class_DeepHit import Model_DeepHit\n",
    "from utils_eval import c_index, brier_score, weighted_c_index, weighted_brier_score\n",
    "\n",
    "\n",
    "def load_logging(filename):\n",
    "    data = dict()\n",
    "    with open(filename) as f:\n",
    "        def is_float(input):\n",
    "            try:\n",
    "                num = float(input)\n",
    "            except ValueError:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key,value = line.strip().split(':', 1)\n",
    "                if value.isdigit():\n",
    "                    data[key] = int(value)\n",
    "                elif is_float(value):\n",
    "                    data[key] = float(value)\n",
    "                elif value == 'None':\n",
    "                    data[key] = None\n",
    "                else:\n",
    "                    data[key] = value\n",
    "            else:\n",
    "                pass # deal with bad lines of text here    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "##### MAIN SETTING\n",
    "OUT_ITERATION               = 5\n",
    "\n",
    "data_mode                   = 'METABRIC' #METABRIC, SYNTHETIC\n",
    "seed                        = 1234\n",
    "\n",
    "EVAL_TIMES                  = [12, 24, 36] # evalution times (for C-index and Brier-Score)\n",
    "\n",
    "\n",
    "##### IMPORT DATASET\n",
    "'''\n",
    "    num_Category            = max event/censoring time * 1.2 (to make enough time horizon)\n",
    "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
    "    max_length              = maximum number of measurements\n",
    "    x_dim                   = data dimension including delta (num_features)\n",
    "    mask1, mask2            = used for cause-specific network (FCNet structure)\n",
    "'''\n",
    "if data_mode == 'SYNTHETIC':\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = impt.import_dataset_SYNTHETIC(norm_mode = 'standard')\n",
    "    EVAL_TIMES  = [12, 24, 36]\n",
    "elif data_mode == 'METABRIC':\n",
    "    (x_dim), (data, time, label), (mask1, mask2) = impt.import_dataset_METABRIC(norm_mode = 'standard')\n",
    "    EVAL_TIMES  = [287, 288, 432]\n",
    "else:\n",
    "    print('ERROR:  DATA_MODE NOT FOUND !!!')\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(mask1)  # dim of mask1: [subj, Num_Event, Num_Category]\n",
    "\n",
    "\n",
    "\n",
    "in_path = data_mode + '/results/'\n",
    "\n",
    "if not os.path.exists(in_path):\n",
    "    os.makedirs(in_path)\n",
    "\n",
    "\n",
    "FINAL1 = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "FINAL2 = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "\n",
    "\n",
    "for out_itr in range(OUT_ITERATION):\n",
    "    in_hypfile = in_path + '/itr_' + str(out_itr) + '/hyperparameters_log.txt'\n",
    "    in_parser = load_logging(in_hypfile)\n",
    "\n",
    "\n",
    "    ##### HYPER-PARAMETERS\n",
    "    mb_size                     = in_parser['mb_size']\n",
    "\n",
    "    iteration                   = in_parser['iteration']\n",
    "\n",
    "    keep_prob                   = in_parser['keep_prob']\n",
    "    lr_train                    = in_parser['lr_train']\n",
    "\n",
    "    h_dim_shared                = in_parser['h_dim_shared']\n",
    "    h_dim_CS                    = in_parser['h_dim_CS']\n",
    "    num_layers_shared           = in_parser['num_layers_shared']\n",
    "    num_layers_CS               = in_parser['num_layers_CS']\n",
    "\n",
    "    if in_parser['active_fn'] == 'relu':\n",
    "        active_fn                = tf.nn.relu\n",
    "    elif in_parser['active_fn'] == 'elu':\n",
    "        active_fn                = tf.nn.elu\n",
    "    elif in_parser['active_fn'] == 'tanh':\n",
    "        active_fn                = tf.nn.tanh\n",
    "    else:\n",
    "        print('Error!')\n",
    "\n",
    "\n",
    "    initial_W                   = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    alpha                       = in_parser['alpha']  #for log-likelihood loss\n",
    "    beta                        = in_parser['beta']  #for ranking loss\n",
    "    gamma                       = in_parser['gamma']  #for RNN-prediction loss\n",
    "    parameter_name              = 'a' + str('%02.0f' %(10*alpha)) + 'b' + str('%02.0f' %(10*beta)) + 'c' + str('%02.0f' %(10*gamma))\n",
    "\n",
    "\n",
    "    ##### MAKE DICTIONARIES\n",
    "    # INPUT DIMENSIONS\n",
    "    input_dims                  = { 'x_dim'         : x_dim,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category}\n",
    "\n",
    "    # NETWORK HYPER-PARMETERS\n",
    "    network_settings            = { 'h_dim_shared'         : h_dim_shared,\n",
    "                                    'h_dim_CS'          : h_dim_CS,\n",
    "                                    'num_layers_shared'    : num_layers_shared,\n",
    "                                    'num_layers_CS'    : num_layers_CS,\n",
    "                                    'active_fn'      : active_fn,\n",
    "                                    'initial_W'         : initial_W }\n",
    "\n",
    "\n",
    "    # for out_itr in range(OUT_ITERATION):\n",
    "    print ('ITR: ' + str(out_itr+1) + ' DATA MODE: ' + data_mode + ' (a:' + str(alpha) + ' b:' + str(beta) + ' c:' + str(gamma) + ')' )\n",
    "    ##### CREATE DEEPFHT NETWORK\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    ##### PREDICTION & EVALUATION\n",
    "    saver.restore(sess, in_path + '/itr_' + str(out_itr) + '/models/model_itr_' + str(out_itr))\n",
    "\n",
    "    ### PREDICTION\n",
    "    pred = model.predict(te_data)\n",
    "    \n",
    "    ### EVALUATION\n",
    "    result1, result2 = np.zeros([num_Event, len(EVAL_TIMES)]), np.zeros([num_Event, len(EVAL_TIMES)])\n",
    "\n",
    "    for t, t_time in enumerate(EVAL_TIMES):\n",
    "        eval_horizon = int(t_time)\n",
    "\n",
    "        if eval_horizon >= num_Category:\n",
    "            print( 'ERROR: evaluation horizon is out of range')\n",
    "            result1[:, t] = result2[:, t] = -1\n",
    "        else:\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n",
    "            for k in range(num_Event):\n",
    "                # result1[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "                # result2[k, t] = brier_score(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "                result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "                result2[k, t] = weighted_brier_score(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "    FINAL1[:, :, out_itr] = result1\n",
    "    FINAL2[:, :, out_itr] = result2\n",
    "\n",
    "    ### SAVE RESULTS\n",
    "    row_header = []\n",
    "    for t in range(num_Event):\n",
    "        row_header.append('Event_' + str(t+1))\n",
    "\n",
    "    col_header1 = []\n",
    "    col_header2 = []\n",
    "    for t in EVAL_TIMES:\n",
    "        col_header1.append(str(t) + 'yr c_index')\n",
    "        col_header2.append(str(t) + 'yr B_score')\n",
    "\n",
    "    # c-index result\n",
    "    df1 = pd.DataFrame(result1, index = row_header, columns=col_header1)\n",
    "    df1.to_csv(in_path + '/result_CINDEX_itr' + str(out_itr) + '.csv')\n",
    "\n",
    "    # brier-score result\n",
    "    df2 = pd.DataFrame(result2, index = row_header, columns=col_header2)\n",
    "    df2.to_csv(in_path + '/result_BRIER_itr' + str(out_itr) + '.csv')\n",
    "\n",
    "    ### PRINT RESULTS\n",
    "    print('========================================================')\n",
    "    print('ITR: ' + str(out_itr+1) + ' DATA MODE: ' + data_mode + ' (a:' + str(alpha) + ' b:' + str(beta) + ' c:' + str(gamma) + ')' )\n",
    "    print('SharedNet Parameters: ' + 'h_dim_shared = '+str(h_dim_shared) + ' num_layers_shared = '+str(num_layers_shared) + 'Non-Linearity: ' + str(active_fn))\n",
    "    print('CSNet Parameters: ' + 'h_dim_CS = '+str(h_dim_CS) + ' num_layers_CS = '+str(num_layers_CS) + 'Non-Linearity: ' + str(active_fn)) \n",
    "\n",
    "    print('--------------------------------------------------------')\n",
    "    print('- C-INDEX: ')\n",
    "    print(df1)\n",
    "    print('--------------------------------------------------------')\n",
    "    print('- BRIER-SCORE: ')\n",
    "    print(df2)\n",
    "    print('========================================================')\n",
    "\n",
    "\n",
    "    \n",
    "### FINAL MEAN/STD\n",
    "# c-index result\n",
    "df1_mean = pd.DataFrame(np.mean(FINAL1, axis=2), index = row_header, columns=col_header1)\n",
    "df1_std  = pd.DataFrame(np.std(FINAL1, axis=2), index = row_header, columns=col_header1)\n",
    "df1_mean.to_csv(in_path + '/result_CINDEX_FINAL_MEAN.csv')\n",
    "df1_std.to_csv(in_path + '/result_CINDEX_FINAL_STD.csv')\n",
    "\n",
    "# brier-score result\n",
    "df2_mean = pd.DataFrame(np.mean(FINAL2, axis=2), index = row_header, columns=col_header2)\n",
    "df2_std  = pd.DataFrame(np.std(FINAL2, axis=2), index = row_header, columns=col_header2)\n",
    "df2_mean.to_csv(in_path + '/result_BRIER_FINAL_MEAN.csv')\n",
    "df2_std.to_csv(in_path + '/result_BRIER_FINAL_STD.csv')\n",
    "\n",
    "\n",
    "### PRINT RESULTS\n",
    "print('========================================================')\n",
    "print('- FINAL C-INDEX: ')\n",
    "print(df1_mean)\n",
    "print('--------------------------------------------------------')\n",
    "print('- FINAL BRIER-SCORE: ')\n",
    "print(df2_mean)\n",
    "print('========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
